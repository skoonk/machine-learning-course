{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "This report details Python code and explanations for Project 1, Predicting Boston Housing Prices. It was created using the Jupyter notebook and implemented using Python 2.7 features. \n",
    "\n",
    "The module, boston_housing.py, can be executed individually without this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Statistical Analysis and Data Exploration\n",
    "\n",
    "The below code snippet will calculate the following characteristics in the data set:\n",
    "\n",
    "* Number of data points and features\n",
    "* Minimum, maximum, mean and median prices of the housing prices\n",
    "* The standard deviation of the housing prices\n",
    "\n",
    "The data, which is built into the sklearn library, is originally from from the StatLib library which is maintained at Carnegie Mellon University. You can read more about the data here:\n",
    "\n",
    "* https://archive.ics.uci.edu/ml/datasets/Housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of data (number of houses): 506\n",
      "Number of features: 13\n",
      "Minimum price: $5.0K\n",
      "Maximum price: $50.0K\n",
      "Mean price: $22.5K\n",
      "Median price: $21.2K\n",
      "Standard deviation: $9.19K\n"
     ]
    }
   ],
   "source": [
    "############################################################\n",
    "# LOAD LIBRARIES\n",
    "############################################################\n",
    "\n",
    "import numpy as np\n",
    "import pylab as pl\n",
    "from sklearn import datasets\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"Load the Boston dataset.\"\"\"\n",
    "\n",
    "    boston = datasets.load_boston()\n",
    "    return boston\n",
    "\n",
    "def explore_city_data(city_data):\n",
    "    \"\"\"Calculate the Boston housing statistics.\"\"\"\n",
    "\n",
    "    # Get the labels and features from the housing data\n",
    "    housing_prices = city_data.target\n",
    "    housing_features = city_data.data\n",
    "    \n",
    "    # Calculate features per project requirements\n",
    "    print 'Size of data (number of houses): ' + str(len(housing_prices))\n",
    "    print 'Number of features: ' + str(housing_features.shape[1])\n",
    "    \n",
    "    # Note: Median value of owner-occupied homes in $1000's. See\n",
    "    # https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.names\n",
    "    print 'Minimum price: $' + str(np.min(housing_prices)) + 'K'\n",
    "    print 'Maximum price: $' + str(np.max(housing_prices)) + 'K'\n",
    "    print 'Mean price: $' + str(round(np.mean(housing_prices),1)) + 'K'\n",
    "    print 'Median price: $' + str(round(np.median(housing_prices),1)) + 'K'\n",
    "    print 'Standard deviation: $' + str(round(np.std(housing_prices),2)) + 'K'\n",
    "    \n",
    "# Load data\n",
    "city_data = load_data()\n",
    "\n",
    "# Explore the data\n",
    "explore_city_data(city_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Evaluating Model Performance\n",
    "## Measure of Model Performance\n",
    "\n",
    "Since this is a regression analysis (and not classification), metrics that are capable of evaluating performance errors for continuous outputs are required. The following performance metrics satisfying these requirements are available in scikit-learn:\n",
    "\n",
    "* Mean absolute error (MAE)\n",
    "* Mean squared error (MSE)\n",
    "* Median absolute error\n",
    "\n",
    "From the above, the median absolute error was selected since it is resistant to the presence of outliers. \n",
    "\n",
    "Note that R-squared is also available for regression; however, it is not a valid error metric in the context of the supplied assignment.\n",
    "\n",
    "## Splitting into Training & Testing Data\n",
    "For all machine learning examples, we want to separate the data into training and test datasets. We use the former to fit the model and the latter to evaluate its performance.\n",
    "\n",
    "Prediction algorithms that do not use split training and test data sets may have high variance error, which is a result of overfitting the dataset. In other words, we can make great predictions using existing data but will make poor predictions when faced with unseen data.\n",
    "\n",
    "## Grid Search & Cross Validation\n",
    "Grid search is a function for tuning the hyperparameters (e.g. maximum depth, C) used in machine learning algorithms. It iterates over different combinations of hyperparameters and determines which combination results in the highest performance.\n",
    "\n",
    "Cross-validation is a technique where some of the data is removed from the dataset before training begins. The removed data is later used to evaluate the performance of the model. \n",
    "\n",
    "Note that by default, grid search uses 3-fold cross-validation. Cross-validation is useful when combined with grid search to ensure that our selected hyperparameters do not result in a model that overfits the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Evaluating Model Performance\n",
    "## Trends of Training & Testing Error\n",
    "As the training size increases, the training and testing errors generally converge to the same absolute error. For example, the two errors nearly converge to an error of about 2 in the below image.\n",
    "\n",
    "<img src='learning-graph-example.png'>\n",
    "\n",
    "## Decision Tree Regressor\n",
    "When the model is fully trained, the decision tree regressor for max depth of 1 suffers from high bias/underfitting since both training and test data set errors converge to the same high value.\n",
    "<img src='learning-graph-example-depth-1.png'>\n",
    "For a max depth of 10, the model suffers from high variance/overfitting since there are no errors with the training data set but high errors with the test data set.\n",
    "<img src='learning-graph-example-depth-10.png'>\n",
    "\n",
    "## Model Complexity\n",
    "The training set error continually decreases to zero as the model complexity increases; however, the test set error plateus to a steady-state value with increasing complexity.\n",
    "\n",
    "From just this graph, a max depth of 5 appears to balance between underfitting and overfitting the model. This is the point where the test data set error starts to plateau and before the test and training data set errors diverge.\n",
    "\n",
    "<img src='model-complexity.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 4) Model Prediction\n",
    "Running the GridSearchCV command several times for:\n",
    "\n",
    "House: [11.95, 0.0, 18.1, 0, 0.659, 5.609, 90.0, 1.385, 24, 680.0, 20.2, 332.09, 12.13]\n",
    "\n",
    "yields a prediction between \\$20K and \\$21K, with optimal models having a max depth between 5 and 7. Regardless, the model appears reasonable and valid since the median was \\$21.2K and the mean was \\$22.5K with a standard deviation of \\$9.19K.\n",
    "\n",
    "The values for the home provided yield an average priced home."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
