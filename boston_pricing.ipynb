{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "This report details Python code and explanations for Project 1, Predicting Boston Housing Prices. It was created using the Jupyter notebook and implemented using Python 2.7 features. \n",
    "\n",
    "The module, boston_housing.py, can be executed individually without this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Statistical Analysis and Data Exploration\n",
    "\n",
    "The below code snippet will calculate the following characteristics in the data set:\n",
    "\n",
    "* Number of data points and features\n",
    "* Minimum, maximum, mean and median prices of the housing prices\n",
    "* The standard deviation of the housing prices\n",
    "\n",
    "The data, which is built into the sklearn library, is originally from from the StatLib library which is maintained at Carnegie Mellon University. You can read more about the data here:\n",
    "\n",
    "* https://archive.ics.uci.edu/ml/datasets/Housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of data (number of houses): 506\n",
      "Number of features: 13\n",
      "Minimum price: $5.0K\n",
      "Maximum price: $50.0K\n",
      "Mean price: $22.5K\n",
      "Median price: $21.2K\n",
      "Standard deviation: $9.19K\n"
     ]
    }
   ],
   "source": [
    "############################################################\n",
    "# LOAD LIBRARIES\n",
    "############################################################\n",
    "\n",
    "import numpy as np\n",
    "import pylab as pl\n",
    "from sklearn import datasets\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"Load the Boston dataset.\"\"\"\n",
    "\n",
    "    boston = datasets.load_boston()\n",
    "    return boston\n",
    "\n",
    "def explore_city_data(city_data):\n",
    "    \"\"\"Calculate the Boston housing statistics.\"\"\"\n",
    "\n",
    "    # Get the labels and features from the housing data\n",
    "    housing_prices = city_data.target\n",
    "    housing_features = city_data.data\n",
    "    \n",
    "    # Calculate features per project requirements\n",
    "    print 'Size of data (number of houses): ' + str(len(housing_prices))\n",
    "    print 'Number of features: ' + str(housing_features.shape[1])\n",
    "    \n",
    "    # Note: Median value of owner-occupied homes in $1000's. See\n",
    "    # https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.names\n",
    "    print 'Minimum price: $' + str(np.min(housing_prices)) + 'K'\n",
    "    print 'Maximum price: $' + str(np.max(housing_prices)) + 'K'\n",
    "    print 'Mean price: $' + str(round(np.mean(housing_prices),1)) + 'K'\n",
    "    print 'Median price: $' + str(round(np.median(housing_prices),1)) + 'K'\n",
    "    print 'Standard deviation: $' + str(round(np.std(housing_prices),2)) + 'K'\n",
    "    \n",
    "# Load data\n",
    "city_data = load_data()\n",
    "\n",
    "# Explore the data\n",
    "explore_city_data(city_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Partition the Data Set\n",
    "The train_test_split function from the cross_validation module within sklearn is used to partition the data into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of X training set:(354, 13)\n",
      "Size of y training set:(354,)\n",
      "Size of X test set:(152, 13)\n",
      "Size of y test set:(152,)\n",
      "Ratio (training set): 0.7\n",
      "Ratio (test set): 0.3\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "def split_data(city_data):\n",
    "    \"\"\"Randomly shuffle the sample set. Divide it into 70 percent training and 30 percent testing data.\"\"\"\n",
    "\n",
    "    # Get the features and labels from the Boston housing data\n",
    "    X, y = city_data.data, city_data.target\n",
    "    \n",
    "    # Split the data into training and test sets (70-30 split)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "# Load data\n",
    "city_data = load_data()\n",
    "\n",
    "# Training/Test dataset split\n",
    "X_train, y_train, X_test, y_test = split_data(city_data)\n",
    "\n",
    "# Print out metrics for splitting training/test data set for verification\n",
    "print 'Size of X training set:' + str(X_train.shape) \n",
    "print 'Size of y training set:' + str(y_train.shape)\n",
    "print 'Size of X test set:' + str(X_test.shape) \n",
    "print 'Size of y test set:' + str(y_test.shape)\n",
    "print 'Ratio (training set): ' + str(round(float(X_train.shape[0])/(X_test.shape[0] + X_train.shape[0]),2))\n",
    "print 'Ratio (test set): ' + str(round(float(X_test.shape[0])/(X_test.shape[0] + X_train.shape[0]),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Performance Metrics\n",
    "\n",
    "Since this is a regression and not a classification model, both the mean squared error (MSE) and coefficient of determination (R^2) can be useful metrics. \n",
    "\n",
    "I've chosen to use R^2 since it is a standardized form of the MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "def performance_metric(label, prediction):\n",
    "    \"\"\"Calculate and return the appropriate error performance metric.\"\"\"\n",
    "\n",
    "    print('R^2 train: %.3f, test: %.3f' %\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Model Performance\n",
    "\n",
    "* Which measure of model performance is best to use for predicting Boston housing data and analyzing the errors? \n",
    "* Why do you think this measurement most appropriate? Why might the other measurements not be appropriate here?\n",
    "* Why is it important to split the Boston housing data into training and testing data? What happens if you do not do this?\n",
    "* What does grid search do and why might you want to use it?\n",
    "* Why is cross validation useful and why might we use it with grid search?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Analyzing Model Performance\n",
    "\n",
    "* Look at all learning curve graphs provided. What is the general trend of training and testing error as training size increases?\n",
    "* Look at the learning curves for the decision tree regressor with max depth 1 and 10 (first and last learning curve graphs). When the model is fully trained does it suffer from either high bias/underfitting or high variance/overfitting?\n",
    "* Look at the model complexity graph. How do the training and test error relate to increasing model complexity? Based on this relationship, which model (max depth) best generalizes the dataset and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Model Prediction\n",
    "\n",
    "* Model makes predicted housing price with detailed model parameters (max depth) reported using grid search. Note due to the small randomization of the code it is recommended to run the program several times to identify the most common/reasonable price/model complexity.\n",
    "* Compare prediction to earlier statistics and make a case if you think it is a valid model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Questions and Report Structure\n",
    "\n",
    "1) Statistical Analysis and Data Exploration\n",
    "\n",
    "Number of data points (houses)?\n",
    "Number of features?\n",
    "Minimum and maximum housing prices?\n",
    "Mean and median Boston housing prices?\n",
    "Standard deviation?\n",
    "\n",
    "3) Analyzing Model Performance\n",
    "\n",
    "Look at all learning curve graphs provided. What is the general trend of training and testing error as training size increases?\n",
    "Look at the learning curves for the decision tree regressor with max depth 1 and 10 (first and last learning curve graphs). When the model is fully trained does it suffer from either high bias/underfitting or high variance/overfitting?\n",
    "Look at the model complexity graph. How do the training and test error relate to increasing model complexity? Based on this relationship, which model (max depth) best generalizes the dataset and why?\n",
    "4) Model Prediction\n",
    "\n",
    "Model makes predicted housing price with detailed model parameters (max depth) reported using grid search. Note due to the small randomization of the code it is recommended to run the program several times to identify the most common/reasonable price/model complexity.\n",
    "Compare prediction to earlier statistics and make a case if you think it is a valid model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Analysis and Data Exploration\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Analyzing Model Performance\n",
    "\n",
    "# Model Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Evaluating Model Performance\n",
    "## Measure of Model Performance\n",
    "\n",
    "Since this is a regression analysis (and not classification), metrics that are capable of evaluating performance errors for continous outputs are required. The following performance metrics satisfying these requirements are available in scikit-learn:\n",
    "\n",
    "* Mean absolute error (MAE)\n",
    "* Mean squared error (MSE)\n",
    "* Median absolute error\n",
    "\n",
    "From the above, the median absolute error was selected since it is resistant to the presence of outliers. \n",
    "\n",
    "Note that R-squared is also available for regression; however, it is not a valid error metric in the context of the supplied assignment.\n",
    "\n",
    "## Splitting into Training & Testing Data\n",
    "For all machine learning examples, we want to separate the data into training and test datasets. We use the former to fit the model and the latter to evaluate its performance.\n",
    "\n",
    "Prediction algorithms that do not use split training and test data sets may have high variance error, which is a result of overfitting the dataset. In other words, we can make great predictions using existing data but will make poor predictions when faced with unseen data.\n",
    "\n",
    "## Grid Search & Cross Validation\n",
    "Grid search is a function for tuning the hyperparameters (e.g. maximum depth, C) used in machine learning algorithms. It iterates over different combinations of hyperparameters and determines which combination results in the highest performance.\n",
    "\n",
    "Cross-validation is a technique where some of the data is removed from the dataset before training begins. The removed data is later used to evaluate the performance of the model. \n",
    "\n",
    "Note that by default, grid search uses 3-fold cross-validation. Cross-validation is useful when combined with grid search to ensure that our selected hyperparameters do not result in a model that overfits the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Evaluating Model Performance\n",
    "## Trends of Training & Testing Error\n",
    "When reviewing all of the learning curves, the general trend of training and testing error is to converge to the same absolute error as the training size increases. For example, a maximum depth of 5 demonstrates this trend between the two errors.\n",
    "\n",
    "<img src='learning-graph-example.png'>\n",
    "\n",
    "## Decision Tree Regressor\n",
    "Look at the learning curves for the decision tree regressor with max depth 1 and 10 (first and last learning curve graphs). When the model is fully trained does it suffer from either high bias/underfitting or high variance/overfitting?\n",
    "\n",
    "## Model Complexity\n",
    "Look at the model complexity graph. How do the training and test error relate to increasing model complexity? Based on this relationship, which model (max depth) best generalizes the dataset and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
